#YamlMime:PythonReference
api_name: []
items:
- _type: module
  children:
  - cntk.losses.binary_cross_entropy
  - cntk.losses.cosine_distance
  - cntk.losses.cosine_distance_with_negative_samples
  - cntk.losses.cross_entropy_with_softmax
  - cntk.losses.lambda_rank
  - cntk.losses.squared_error
  - cntk.losses.weighted_binary_cross_entropy
  fullName: cntk.losses
  langs:
  - python
  module: cntk.losses
  name: losses
  source:
    id: losses
    path: \cntk\losses\__init__.py
    remote:
      branch: master
      path: \cntk\losses\__init__.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 0
  type: Namespace
  uid: cntk.losses
- _type: function
  fullName: cntk.losses.binary_cross_entropy
  langs:
  - python
  module: cntk.losses
  name: binary_cross_entropy
  source:
    id: binary_cross_entropy
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 90
  summary: 'Computes the binary cross entropy (aka logistic loss) between the `output`
    and `target`.

    -[ Example ]-

    TBA'
  syntax:
    content: binary_cross_entropy()
    parameters:
    - description: the computed posterior probability for a variable to be 1 from
        the network (typ. a `sigmoid`)
      id: output
    - description: ground-truth label, 0 or 1
      id: target
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '`Function`'
  type: Method
  uid: cntk.losses.binary_cross_entropy
- _type: function
  fullName: cntk.losses.cosine_distance
  langs:
  - python
  module: cntk.losses
  name: cosine_distance
  source:
    id: cosine_distance
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 18
  summary: "Computes the cosine distance between `x` and `y`:\n-[ Example ]-\n>>>\
    \ a = np.asarray([-1, -1, -1, 1, 1, -1, 1, 1, -1, 1, 1, -1]).reshape(3,2,2)\n\
    >>> b = np.asarray([1, 1, -1, 1, 1, -1, 1, -1, -1, -1, -1, 1]).reshape(3,2,2)\n\
    >>> x = C.sequence.input(shape=(2,))\n>>> y = C.sequence.input(shape=(2,))\n>>>\
    \ np.round(C.cosine_distance(x,y).eval({x:a,y:b}),5)\narray([[-1.,  1.],\n   \
    \    [ 1.,  0.],\n       [ 0., -1.]], dtype=float32)"
  syntax:
    content: cosine_distance()
    parameters:
    - description: numpy array or any `Function` that outputs a tensor
      id: x
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '`Function`'
  type: Method
  uid: cntk.losses.cosine_distance
- _type: function
  fullName: cntk.losses.cosine_distance_with_negative_samples
  langs:
  - python
  module: cntk.losses
  name: cosine_distance_with_negative_samples
  source:
    id: cosine_distance_with_negative_samples
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 47
  summary: "Given minibatches for `x` and `y`, this function computes for each element\
    \ in *x* the cosine distance between it and the corresponding *y* and additionally\
    \ the cosine distance between `x` and some other elements of `y` (referred to\
    \ a negative samples). The `x` and `y` pairs are samples often derived from embeddings\
    \ of textual data, though the function can be used for any form of numeric encodings.\
    \ When using this function to compute textual similarity, `x` represents search\
    \ query term embedding and `y` represents a document embedding. The negative samples\
    \ are formed on the fly by shifting the right side (`y`). The `shift` indicates\
    \ how many samples in `y` one should shift while forming each negative sample\
    \ pair. It is often chosen to be 1. As the name suggests `num_negative_samples`\
    \ indicates how many negative samples one would want to generate.\n-[ Example\
    \ ]-\n>>> qry = np.asarray([1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1.], dtype=np.float32).reshape(3,\
    \ 1, 4)\n>>> doc = np.asarray([1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1.],\
    \ dtype=np.float32).reshape(3, 1, 4)\n>>> x = C.sequence.input(shape=(4,))\n>>>\
    \ y = C.sequence.input(shape=(4,))\n>>> model = C.cosine_distance_with_negative_samples(x,\
    \ y, shift=1, num_negative_samples=2)\n>>> np.round(model.eval({x: qry, y: doc}),\
    \ decimals=4)\narray([[[ 1. ,  0.5,  0. ]],\n<BLANKLINE>\n       [[ 1. ,  0.5,\
    \  0.5]],\n<BLANKLINE>\n       [[ 1. ,  0. ,  0.5]]], dtype=float32)"
  syntax:
    content: cosine_distance_with_negative_samples()
    parameters:
    - description: numpy array or any `Function` that outputs a tensor
      id: x
    - description: numpy array or any `Function` that outputs a tensor
      id: y
    - description: non-zero positive integer representing number of shift to generate
        a negative sample
      id: shift
    - description: number of negative samples to generate, a non-zero positive integer
      id: num_negative_samples
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '`Function`'
  type: Method
  uid: cntk.losses.cosine_distance_with_negative_samples
- _type: function
  fullName: cntk.losses.cross_entropy_with_softmax
  langs:
  - python
  module: cntk.losses
  name: cross_entropy_with_softmax
  source:
    id: cross_entropy_with_softmax
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 134
  summary: 'This operation computes the cross entropy between the `target_vector`
    and the softmax of the `output_vector`. The elements of `target_vector` have to
    be non-negative and should sum to 1. The `output_vector` can contain any values.
    The function will internally compute the softmax of the `output_vector`. Concretely,



    with the understanding that the implementation can use equivalent formulas for
    efficiency and numerical stability.

    -[ Example ]-

    >>> C.cross_entropy_with_softmax([[1., 1., 1., 50.]], [[0., 0., 0., 1.]]).eval()

    array([[ 0.]], dtype=float32)

    >>> C.cross_entropy_with_softmax([[1., 2., 3., 4.]], [[0.35, 0.15, 0.05, 0.45]]).eval()

    array([[ 1.84019]], dtype=float32)'
  syntax:
    content: cross_entropy_with_softmax()
    parameters:
    - description: the unscaled computed output values from the network
      id: output_vector
    - description: usually it is one-hot vector where the hot bit corresponds to the
        label index. But it can be any probability distribution over the labels.
      id: target_vector
    - description: if given, cross entropy will be computed along this axis
      id: axis
      type:
      - int or Axis, optional
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '`Function`'
  type: Method
  uid: cntk.losses.cross_entropy_with_softmax
- _type: function
  fullName: cntk.losses.lambda_rank
  langs:
  - python
  module: cntk.losses
  name: lambda_rank
  source:
    id: lambda_rank
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 206
  summary: "Groups samples according to `group`, sorts them within each group based\
    \ on `output` and computes the Normalized Discounted Cumulative Gain (NDCG) at\
    \ infinity for each group. Concretely, the Discounted Cumulative Gain (DCG) at\
    \ infinity is:\n\nwhere  means the gain of the -th ranked sample.\nThe NDCG is\
    \ just the DCG  divided by the maximum achievable DCG (obtained by placing the\
    \ samples with the largest gain at the top of the ranking).\nSamples in the same\
    \ group must appear in order of decreasing gain.\nIt returns 1 minus the average\
    \ NDCG across all the groups in the minibatch multiplied by 100 times the number\
    \ of samples in the minibatch.\nIn the backward direction it back-propagates LambdaRank\
    \ gradients.\n-[ Example ]-\n>>> group = C.input((1,))\n>>> score = C.input((1,),\
    \ needs_gradient=True)\n>>> gain  = C.input((1,))\n>>> g = np.array([1, 1, 2,\
    \ 2], dtype=np.float32).reshape(4,1)\n>>> s = np.array([1, 2, 3, 4], dtype=np.float32).reshape(4,1)\n\
    >>> n = np.array([7, 1, 3, 1], dtype=np.float32).reshape(4,1)\n>>> f = C.lambda_rank(score,\
    \ gain, group)\n>>> np.round(f.grad({score:s, gain:n, group: g}, wrt=[score]),4)\n\
    array([[-0.2121],\n<BLANKLINE>\n       [ 0.2121],\n<BLANKLINE>\n       [-0.1486],\n\
    <BLANKLINE>\n       [ 0.1486]], dtype=float32)"
  syntax:
    content: lambda_rank()
    parameters:
    - description: score of each sample
      id: output
    - description: gain of each sample
      id: gain
    - description: group of each sample
      id: group
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '`Function`'
  type: Method
  uid: cntk.losses.lambda_rank
- _type: function
  fullName: cntk.losses.squared_error
  langs:
  - python
  module: cntk.losses
  name: squared_error
  source:
    id: squared_error
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 176
  summary: 'This operation computes the sum of the squared difference between elements
    in the two input matrices. The result is a scalar (i.e., one by one matrix). This
    is often used as a training criterion.

    -[ Example ]-

    >>> i1 = C.input((1,2))

    >>> i2 = C.input((1,2))

    >>> C.squared_error(i1,i2).eval({i1:np.asarray([[[2., 1.]]], dtype=np.float32),
    i2:np.asarray([[[4., 6.]]], dtype=np.float32)})

    array([ 29.], dtype=float32)

    >>> C.squared_error(i1,i2).eval({i1:np.asarray([[[1., 2.]]], dtype=np.float32),
    i2:np.asarray([[[1., 2.]]], dtype=np.float32)})

    array([ 0.], dtype=float32)'
  syntax:
    content: squared_error()
    parameters:
    - description: the output values from the network
      id: output
    - description: it is usually a one-hot vector where the hot bit corresponds to
        the label index
      id: target
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '`Function`'
  type: Method
  uid: cntk.losses.squared_error
- _type: function
  fullName: cntk.losses.weighted_binary_cross_entropy
  langs:
  - python
  module: cntk.losses
  name: weighted_binary_cross_entropy
  source:
    id: weighted_binary_cross_entropy
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 111
  summary: 'This operation computes the weighted binary cross entropy (aka logistic
    loss) between the `output` and `target`.

    -[ Example ]-

    TBA'
  syntax:
    content: weighted_binary_cross_entropy()
    parameters:
    - description: the computed posterior probability from the network
      id: output
    - description: ground-truth label, 0 or 1
      id: target
    - description: weight of each example
      id: weight
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, optional
    return:
      description: '`Function`'
  type: Method
  uid: cntk.losses.weighted_binary_cross_entropy
references:
- fullName: cntk.losses.binary_cross_entropy
  isExternal: false
  name: binary_cross_entropy
  parent: cntk.losses
  uid: cntk.losses.binary_cross_entropy
- fullName: cntk.losses.cosine_distance
  isExternal: false
  name: cosine_distance
  parent: cntk.losses
  uid: cntk.losses.cosine_distance
- fullName: cntk.losses.cosine_distance_with_negative_samples
  isExternal: false
  name: cosine_distance_with_negative_samples
  parent: cntk.losses
  uid: cntk.losses.cosine_distance_with_negative_samples
- fullName: cntk.losses.cross_entropy_with_softmax
  isExternal: false
  name: cross_entropy_with_softmax
  parent: cntk.losses
  uid: cntk.losses.cross_entropy_with_softmax
- fullName: cntk.losses.lambda_rank
  isExternal: false
  name: lambda_rank
  parent: cntk.losses
  uid: cntk.losses.lambda_rank
- fullName: cntk.losses.squared_error
  isExternal: false
  name: squared_error
  parent: cntk.losses
  uid: cntk.losses.squared_error
- fullName: cntk.losses.weighted_binary_cross_entropy
  isExternal: false
  name: weighted_binary_cross_entropy
  parent: cntk.losses
  uid: cntk.losses.weighted_binary_cross_entropy
