#YamlMime:PythonReference
api_name: []
items:
- _type: module
  children:
  - cntk.layers.blocks.BlockFunction
  - cntk.layers.blocks.ForwardDeclaration
  - cntk.layers.blocks.GRU
  - cntk.layers.blocks.LSTM
  - cntk.layers.blocks.RNNUnit
  - cntk.layers.blocks.Stabilizer
  - cntk.layers.blocks.UntestedBranchError
  fullName: cntk.layers.blocks
  langs:
  - python
  module: cntk.layers.blocks
  name: blocks
  source:
    id: blocks
    path: \cntk\layers\blocks.py
    remote:
      branch: master
      path: \cntk\layers\blocks.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 0
  summary: Basic building blocks that are semantically not layers (not used in a layered
    fashion), e.g. the LSTM block.
  type: Namespace
  uid: cntk.layers.blocks
- _type: function
  fullName: cntk.layers.blocks.BlockFunction
  langs:
  - python
  module: cntk.layers.blocks
  name: BlockFunction
  source:
    id: BlockFunction
    path: \cntk\layers\blocks.py
    remote:
      branch: master
      path: \cntk\layers\blocks.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 69
  summary: Decorator for defining a @Function as a BlockFunction. Same as @Function,
    but wrap the content into an `as_block()`.
  syntax:
    content: BlockFunction(op_name, name)
    parameters:
    - id: op_name
    - id: name
  type: Method
  uid: cntk.layers.blocks.BlockFunction
- _type: function
  fullName: cntk.layers.blocks.ForwardDeclaration
  langs:
  - python
  module: cntk.layers.blocks
  name: ForwardDeclaration
  source:
    id: ForwardDeclaration
    path: \cntk\layers\blocks.py
    remote:
      branch: master
      path: \cntk\layers\blocks.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 88
  summary: "Helper for recurrent network declarations. Returns a placeholder variable\
    \ with an added method `resolve_to()` to be called at the end to close the loop.\
    \ This is used for explicit graph building with recurrent connections.\n-[ Example\
    \ ]-\n>>> # create a graph with a recurrent loop to compute the length of an input\
    \ sequence\n>>> from cntk.layers.typing import *\n>>> x = C.input(**Sequence[Tensor[2]])\n\
    >>> ones_like_input = sequence.broadcast_as(1, x)  # sequence of scalar ones of\
    \ same length as input\n>>> out_fwd = ForwardDeclaration()  # placeholder for\
    \ the state variables\n>>> out = sequence.past_value(out_fwd, initial_state=0)\
    \ + ones_like_input\n>>> out_fwd.resolve_to(out)\n>>> length = sequence.last(out)\n\
    >>> x0 = np.reshape(np.arange(6,dtype=np.float32),(1,3,2))\n>>> x0\n    array([[[\
    \ 0.,  1.],\n            [ 2.,  3.],\n            [ 4.,  5.]]], dtype=float32)\n\
    >>> length(x0)\n    array([[ 3.]], dtype=float32)"
  syntax:
    content: ForwardDeclaration(name="forward_declaration")
    parameters:
    - id: name
    return:
      description: a placeholder variable with a method `resolve_to()` that resolves
        it to another variable
  type: Method
  uid: cntk.layers.blocks.ForwardDeclaration
- _type: function
  fullName: cntk.layers.blocks.GRU
  langs:
  - python
  module: cntk.layers.blocks
  name: GRU
  source:
    id: GRU
    path: \cntk\layers\blocks.py
    remote:
      branch: master
      path: \cntk\layers\blocks.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 445
  summary: 'Layer factory function to create a GRU block for use inside a recurrence.
    The GRU block implements one step of the recurrence and is stateless. It accepts
    the previous state as its first argument, and outputs its new state.

    -[ Example ]-

    >>> # a gated recurrent layer

    >>> from cntk.layers import *

    >>> gru_layer = Recurrence(GRU(500))'
  syntax:
    content: GRU(shape, cell_shape=None, activation=<cntk.default_options.default_override_or
      object at 0x000001E8743EE240>, init=<cntk.default_options.default_override_or
      object at 0x000001E8743EE2B0>, init_bias=<cntk.default_options.default_override_or
      object at 0x000001E8743EE2E8>, enable_self_stabilization=<cntk.default_options.default_override_or
      object at 0x000001E8743EE320>, name="")
    parameters:
    - description: vector or tensor dimension of the output of this layer
      id: shape
      type:
      - int or tuple of ints
    - defaultValue: ''
      description: if given, then the output state is first computed at *cell_shape*
        and linearly projected to *shape*
      id: cell_shape
      type:
      - tuple, defaults to None
    - defaultValue: <cntk.default_options.default_override_or object at 0x000001E8743EE320>
      description: function to apply at the end, e.g. *relu*
      id: activation
      type:
      - Function, defaults to tanh()
    - defaultValue: <cntk.default_options.default_override_or object at 0x000001E8743EE2E8>
      description: initial value of weights *W*
      id: init
      type:
      - scalar or NumPy array or cntk.initializer, defaults to glorot_uniform
    - defaultValue: <cntk.default_options.default_override_or object at 0x000001E8743EE2B0>
      description: initial value of weights *b*
      id: init_bias
      type:
      - scalar or NumPy array or cntk.initializer, defaults to 0
    - defaultValue: <cntk.default_options.default_override_or object at 0x000001E8743EE240>
      description: if *True* then add a `Stabilizer()` to all state-related projections
        (but not the data input)
      id: enable_self_stabilization
      type:
      - bool, defaults to False
    - defaultValue: None
      description: the name of the Function instance in the network
      id: name
      type:
      - str, defaults to ''
    return:
      description: A function `(prev_h, input) -> h` that implements one step of a
        recurrent GRU layer.
      type:
      - cntk.ops.functions.Function
  type: Method
  uid: cntk.layers.blocks.GRU
- _type: function
  fullName: cntk.layers.blocks.LSTM
  langs:
  - python
  module: cntk.layers.blocks
  name: LSTM
  source:
    id: LSTM
    path: \cntk\layers\blocks.py
    remote:
      branch: master
      path: \cntk\layers\blocks.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 358
  summary: 'Layer factory function to create an LSTM block for use inside a recurrence.
    The LSTM block implements one step of the recurrence and is stateless. It accepts
    the previous state as its first two arguments, and outputs its new state as a
    two-valued tuple `(h,c)`.

    -[ Example ]-

    >>> # a typical recurrent LSTM layer

    >>> from cntk.layers import *

    >>> lstm_layer = Recurrence(LSTM(500))'
  syntax:
    content: LSTM(shape, cell_shape=None, activation=<cntk.default_options.default_override_or
      object at 0x000001E8743E9E10>, use_peepholes=<cntk.default_options.default_override_or
      object at 0x000001E8743E9FD0>, init=<cntk.default_options.default_override_or
      object at 0x000001E8743EE080>, init_bias=<cntk.default_options.default_override_or
      object at 0x000001E8743EE048>, enable_self_stabilization=<cntk.default_options.default_override_or
      object at 0x000001E8743EE0F0>, name="")
    parameters:
    - description: vector or tensor dimension of the output of this layer
      id: shape
      type:
      - int or tuple of ints
    - defaultValue: ''
      description: if given, then the output state is first computed at *cell_shape*
        and linearly projected to *shape*
      id: cell_shape
      type:
      - tuple, defaults to None
    - defaultValue: <cntk.default_options.default_override_or object at 0x000001E8743EE0F0>
      description: function to apply at the end, e.g. *relu*
      id: activation
      type:
      - Function, defaults to tanh()
    - defaultValue: <cntk.default_options.default_override_or object at 0x000001E8743EE048>
      description: ''
      id: use_peepholes
      type:
      - bool, defaults to False
    - defaultValue: <cntk.default_options.default_override_or object at 0x000001E8743EE080>
      description: initial value of weights *W*
      id: init
      type:
      - scalar or NumPy array or cntk.initializer, defaults to glorot_uniform
    - defaultValue: <cntk.default_options.default_override_or object at 0x000001E8743E9FD0>
      description: initial value of weights *b*
      id: init_bias
      type:
      - scalar or NumPy array or cntk.initializer, defaults to 0
    - defaultValue: <cntk.default_options.default_override_or object at 0x000001E8743E9E10>
      description: if *True* then add a `Stabilizer()` to all state-related projections
        (but not the data input)
      id: enable_self_stabilization
      type:
      - bool, defaults to False
    - defaultValue: None
      description: the name of the Function instance in the network
      id: name
      type:
      - str, defaults to ''
    return:
      description: A function `(prev_h, prev_c, input) -> (h, c)` that implements
        one step of a recurrent LSTM layer.
      type:
      - cntk.ops.functions.Function
  type: Method
  uid: cntk.layers.blocks.LSTM
- _type: function
  fullName: cntk.layers.blocks.RNNUnit
  langs:
  - python
  module: cntk.layers.blocks
  name: RNNUnit
  source:
    id: RNNUnit
    path: \cntk\layers\blocks.py
    remote:
      branch: master
      path: \cntk\layers\blocks.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 403
  summary: 'Layer factory function to create a plain RNN block for use inside a recurrence.
    The RNN block implements one step of the recurrence and is stateless. It accepts
    the previous state as its first argument, and outputs its new state.

    -[ Example ]-

    >>> # a plain relu RNN layer

    >>> from cntk.layers import *

    >>> relu_rnn_layer = Recurrence(RNNUnit(500, activation=C.relu))'
  syntax:
    content: RNNUnit(shape, cell_shape=None, activation=<cntk.default_options.default_override_or
      object at 0x000001E8743EE128>, init=<cntk.default_options.default_override_or
      object at 0x000001E8743EE198>, init_bias=<cntk.default_options.default_override_or
      object at 0x000001E8743EE1D0>, enable_self_stabilization=<cntk.default_options.default_override_or
      object at 0x000001E8743EE208>, name="")
    parameters:
    - description: vector or tensor dimension of the output of this layer
      id: shape
      type:
      - int or tuple of ints
    - defaultValue: ''
      description: if given, then the output state is first computed at *cell_shape*
        and linearly projected to *shape*
      id: cell_shape
      type:
      - tuple, defaults to None
    - defaultValue: <cntk.default_options.default_override_or object at 0x000001E8743EE208>
      description: function to apply at the end, e.g. *relu*
      id: activation
      type:
      - Function, defaults to signmoid
    - defaultValue: <cntk.default_options.default_override_or object at 0x000001E8743EE1D0>
      description: initial value of weights *W*
      id: init
      type:
      - scalar or NumPy array or cntk.initializer, defaults to glorot_uniform
    - defaultValue: <cntk.default_options.default_override_or object at 0x000001E8743EE198>
      description: initial value of weights *b*
      id: init_bias
      type:
      - scalar or NumPy array or cntk.initializer, defaults to 0
    - defaultValue: <cntk.default_options.default_override_or object at 0x000001E8743EE128>
      description: if *True* then add a `Stabilizer()` to all state-related projections
        (but not the data input)
      id: enable_self_stabilization
      type:
      - bool, defaults to False
    - defaultValue: None
      description: the name of the Function instance in the network
      id: name
      type:
      - str, defaults to ''
    return:
      description: A function `(prev_h, input) -> h` where `h = activation(input @
        W + prev_h @ R + b)`
      type:
      - cntk.ops.functions.Function
  type: Method
  uid: cntk.layers.blocks.RNNUnit
- _type: function
  fullName: cntk.layers.blocks.Stabilizer
  langs:
  - python
  module: cntk.layers.blocks
  name: Stabilizer
  source:
    id: Stabilizer
    path: \cntk\layers\blocks.py
    remote:
      branch: master
      path: \cntk\layers\blocks.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 142
  summary: 'Layer factory function to create a [Droppo self-stabilizer](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/SelfLR.pdf).
    It multiplies its input with a scalar that is learned.

    This takes *enable_self_stabilization* as a flag that allows to disable itself.
    Useful if this is a global default.

    Note: Some other layers (specifically, recurrent units like @cntk.layers.blocks.LSTM)
    also have the option to use the `Stabilizer()` layer internally. That is enabled
    by passing *enable_self_stabilization=True* to those layers. In conjunction with
    those, the rule is that an explicit `Stabilizer()` must be inserted by the user
    for the main data input, whereas the recurrent layer will own the stabilizer(s)
    for the internal recurrent connection(s). Note: Unlike the original paper, which
    proposed a linear or exponential scalar, CNTK uses a sharpened Softplus: 1/steepness
    ln(1+e^{steepness*beta}). The softplus behaves linear for weights around and above
    1 (like the linear scalar) while guaranteeing positiveness (like the exponentional
    variant) but is also more robust by avoiding exploding gradients.

    -[ Example ]-

    >>> # recurrent model with self-stabilization

    >>> from cntk.layers import *

    >>> with default_options(enable_self_stabilization=True): # enable stabilizers
    by default for LSTM()

    ...     model = Sequential([

    ...         Embedding(300),

    ...         Stabilizer(),           # stabilizer for main data input of recurrence

    ...         Recurrence(LSTM(512)),  # LSTM owns its own stabilizers for the recurrent
    connections

    ...         Stabilizer(),

    ...         Dense(10)

    ...     ])'
  syntax:
    content: Stabilizer(steepness=4, enable_self_stabilization=<cntk.default_options.default_override_or
      object at 0x000001E8743E9F60>, name="")
    parameters:
    - defaultValue: <cntk.default_options.default_override_or object at 0x000001E8743E9F60>
      description: ''
      id: steepness
      type:
      - int, defaults to 4
    - defaultValue: '4'
      description: a flag that allows to disable itself. Useful if this is a global
        default
      id: enable_self_stabilization
      type:
      - bool, defaults to False
    - description: the name of the Function instance in the network
      id: name
      type:
      - str, defaults to ''
    return:
      description: A function
      type:
      - cntk.ops.functions.Function
  type: Method
  uid: cntk.layers.blocks.Stabilizer
- _type: function
  fullName: cntk.layers.blocks.UntestedBranchError
  langs:
  - python
  module: cntk.layers.blocks
  name: UntestedBranchError
  source:
    id: UntestedBranchError
    path: \cntk\layers\blocks.py
    remote:
      branch: master
      path: \cntk\layers\blocks.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 29
  syntax:
    content: UntestedBranchError(name)
    parameters:
    - id: name
  type: Method
  uid: cntk.layers.blocks.UntestedBranchError
references:
- fullName: cntk.layers.blocks.BlockFunction
  isExternal: false
  name: BlockFunction
  parent: cntk.layers.blocks
  uid: cntk.layers.blocks.BlockFunction
- fullName: cntk.layers.blocks.ForwardDeclaration
  isExternal: false
  name: ForwardDeclaration
  parent: cntk.layers.blocks
  uid: cntk.layers.blocks.ForwardDeclaration
- fullName: cntk.layers.blocks.GRU
  isExternal: false
  name: GRU
  parent: cntk.layers.blocks
  uid: cntk.layers.blocks.GRU
- fullName: cntk.layers.blocks.LSTM
  isExternal: false
  name: LSTM
  parent: cntk.layers.blocks
  uid: cntk.layers.blocks.LSTM
- fullName: cntk.layers.blocks.RNNUnit
  isExternal: false
  name: RNNUnit
  parent: cntk.layers.blocks
  uid: cntk.layers.blocks.RNNUnit
- fullName: cntk.layers.blocks.Stabilizer
  isExternal: false
  name: Stabilizer
  parent: cntk.layers.blocks
  uid: cntk.layers.blocks.Stabilizer
- fullName: cntk.layers.blocks.UntestedBranchError
  isExternal: false
  name: UntestedBranchError
  parent: cntk.layers.blocks
  uid: cntk.layers.blocks.UntestedBranchError
