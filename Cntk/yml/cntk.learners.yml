#YamlMime:PythonReference
api_name: []
items:
- _type: module
  children:
  - cntk.learners.Learner
  - cntk.learners.UnitType
  - cntk.learners.UserLearner
  - cntk.learners.adadelta
  - cntk.learners.adagrad
  - cntk.learners.adam
  - cntk.learners.default_unit_gain_value
  - cntk.learners.default_use_mean_gradient_value
  - cntk.learners.fsadagrad
  - cntk.learners.learning_rate_schedule
  - cntk.learners.momentum_as_time_constant_schedule
  - cntk.learners.momentum_schedule
  - cntk.learners.momentum_sgd
  - cntk.learners.nesterov
  - cntk.learners.rmsprop
  - cntk.learners.set_default_unit_gain_value
  - cntk.learners.set_default_use_mean_gradient_value
  - cntk.learners.sgd
  - cntk.learners.training_parameter_schedule
  - cntk.learners.Learner
  - cntk.learners.UnitType
  - cntk.learners.UserLearner
  - cntk.learners.adadelta
  - cntk.learners.adagrad
  - cntk.learners.adam
  - cntk.learners.default_unit_gain_value
  - cntk.learners.default_use_mean_gradient_value
  - cntk.learners.fsadagrad
  - cntk.learners.learning_rate_schedule
  - cntk.learners.momentum_as_time_constant_schedule
  - cntk.learners.momentum_schedule
  - cntk.learners.momentum_sgd
  - cntk.learners.nesterov
  - cntk.learners.rmsprop
  - cntk.learners.set_default_unit_gain_value
  - cntk.learners.set_default_use_mean_gradient_value
  - cntk.learners.sgd
  - cntk.learners.training_parameter_schedule
  fullName: cntk.learners
  langs:
  - python
  module: cntk.learners
  name: learners
  source:
    id: learners
    path: \cntk\learners\__init__.py
    remote:
      branch: master
      path: \cntk\learners\__init__.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 0
  summary: 'Learner tunes a set of parameters during the training process. One can
    use different learners for different sets of parameters. Currently, CNTK supports
    the following learning algorithms:


    +------------------------+ | Learning algorithms    | +========================+
    | AdaDelta               | +------------------------+ | AdaGrad                |
    +------------------------+ | FSAdaGrad              | +------------------------+
    | Adam                   | +------------------------+ | MomentumSGD            |
    +------------------------+ | Nesterov               | +------------------------+
    | RMSProp                | +------------------------+ | SGD                    |
    +------------------------+'
  type: Namespace
  uid: cntk.learners
- _type: function
  fullName: cntk.learners.adadelta
  langs:
  - python
  module: cntk.learners
  name: adadelta
  seealsoContent: 'See also: [1]  Matthew D. Zeiler1, [ADADELTA: AN ADAPTIVE LEARNING
    RATE METHOD](https://arxiv.org/pdf/1212.5701.pdf).'
  source:
    id: adadelta
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 582
  summary: Creates an AdaDelta learner instance to learn the parameters. See [1] for
    more information.
  syntax:
    content: adadelta()
    parameters: &id001
    - description: list of network parameters to tune. These can be obtained by the
        root operator's `parameters`.
      id: parameters
      type:
      - list of parameters
    - description: learning rate schedule.
      id: lr
      type:
      - output of learning_rate_schedule()
    - description: exponential smooth factor for each minibatch.
      id: rho
      type:
      - float
    - description: epsilon for sqrt.
      id: epsilon
      type:
      - float
    - description: the L1 regularization weight per sample, defaults to 0.0
      id: l1_regularization_weight
      type:
      - float, optional
    - description: the L2 regularization weight per sample, defaults to 0.0
      id: l2_regularization_weight
      type:
      - float, optional
    - description: the standard deviation of the Gaussian noise added to parameters
        post update, defaults to 0.0
      id: gaussian_noise_injection_std_dev
      type:
      - float, optional
    - description: clipping threshold per sample, defaults to infinity
      id: gradient_clipping_threshold_per_sample
      type:
      - float, optional
    - description: use gradient clipping with truncation
      id: gradient_clipping_with_truncation
      type:
      - bool, default True
    - description: use averaged gradient as input to learner. Defaults to the value
        returned by [default_use_mean_gradient_value()](back_rst/cntk.learners.html.md#cntk.learners.default_use_mean_gradient_value.md).
      id: use_mean_gradient
      type:
      - bool, default False
    return: &id002
      description: Instance of a @cntk.learners.Learner that can be passed to the
        `Trainer`
  type: Method
  uid: cntk.learners.adadelta
- _type: function
  fullName: cntk.learners.adagrad
  langs:
  - python
  module: cntk.learners
  name: adagrad
  seealsoContent: 'See also: [1]  J. Duchi, E. Hazan, and Y. Singer. [Adaptive Subgradient
    Methods for Online Learning and Stochastic Optimization](http://www.magicbroom.info/Papers/DuchiHaSi10.pdf).
    The Journal of Machine Learning Research, 2011.'
  source:
    id: adagrad
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 633
  summary: Creates an AdaGrad learner instance to learn the parameters. See [1] for
    more information.
  syntax:
    content: adagrad()
    parameters: &id003
    - description: list of network parameters to tune. These can be obtained by the
        root operator's `parameters`.
      id: parameters
      type:
      - list of parameters
    - description: learning rate schedule.
      id: lr
      type:
      - output of learning_rate_schedule()
    - description: ''
      id: need_ave_multiplier
      type:
      - bool, default
    - description: the L1 regularization weight per sample, defaults to 0.0
      id: l1_regularization_weight
      type:
      - float, optional
    - description: the L2 regularization weight per sample, defaults to 0.0
      id: l2_regularization_weight
      type:
      - float, optional
    - description: the standard deviation of the Gaussian noise added to parameters
        post update, defaults to 0.0
      id: gaussian_noise_injection_std_dev
      type:
      - float, optional
    - description: clipping threshold per sample, defaults to infinity
      id: gradient_clipping_threshold_per_sample
      type:
      - float, optional
    - description: use gradient clipping with truncation
      id: gradient_clipping_with_truncation
      type:
      - bool, default True
    - description: use averaged gradient as input to learner. Defaults to the value
        returned by [default_use_mean_gradient_value()](back_rst/cntk.learners.html.md#cntk.learners.default_use_mean_gradient_value.md).
      id: use_mean_gradient
      type:
      - bool, default False
    return: &id004
      description: Instance of a @cntk.learners.Learner that can be passed to the
        `Trainer`
  type: Method
  uid: cntk.learners.adagrad
- _type: function
  fullName: cntk.learners.adam
  langs:
  - python
  module: cntk.learners
  name: adam
  seealsoContent: 'See also: [1] D. Kingma, J. Ba. [Adam: A Method for Stochastic
    Optimization](https://arxiv.org/abs/1412.6980). International Conference for Learning
    Representations, 2015.'
  source:
    id: adam
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 741
  summary: Creates an Adam learner instance to learn the parameters. See [1] for more
    information.
  syntax:
    content: adam()
    parameters: &id005
    - description: list of network parameters to tune. These can be obtained by the
        root operator's `parameters`.
      id: parameters
      type:
      - list of parameters
    - description: learning rate schedule.
      id: lr
      type:
      - output of learning_rate_schedule()
    - description: momentum schedule. For additional information, please refer to
        the >>:cntkwiki:`this CNTK Wiki article <BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits>`<<.
      id: momentum
      type:
      - output of momentum_schedule() or momentum_as_time_constant_schedule()
    - description: when `True`, momentum is interpreted as a unit-gain filter. Defaults
        to the value returned by [default_unit_gain_value()](back_rst/cntk.learners.html.md#cntk.learners.default_unit_gain_value.md).
      id: unit_gain
    - description: variance momentum schedule. Defaults to `momentum_as_time_constant_schedule(720000)`.
      id: variance_momentum
      type:
      - output of momentum_schedule() or momentum_as_time_constant_schedule()
    - description: the L1 regularization weight per sample, defaults to 0.0
      id: l1_regularization_weight
      type:
      - float, optional
    - description: the L2 regularization weight per sample, defaults to 0.0
      id: l2_regularization_weight
      type:
      - float, optional
    - description: the standard deviation of the Gaussian noise added to parameters
        post update, defaults to 0.0
      id: gaussian_noise_injection_std_dev
      type:
      - float, optional
    - description: clipping threshold per sample, defaults to infinity
      id: gradient_clipping_threshold_per_sample
      type:
      - float, optional
    - description: use gradient clipping with truncation
      id: gradient_clipping_with_truncation
      type:
      - bool, default True
    - description: use averaged gradient as input to learner. Defaults to the value
        returned by [default_use_mean_gradient_value()](back_rst/cntk.learners.html.md#cntk.learners.default_use_mean_gradient_value.md).
      id: use_mean_gradient
      type:
      - bool, default False
    return: &id006
      description: Instance of a @cntk.learners.Learner that can be passed to the
        `Trainer`
  type: Method
  uid: cntk.learners.adam
- _type: function
  fullName: cntk.learners.default_unit_gain_value
  langs:
  - python
  module: cntk.learners
  name: default_unit_gain_value
  source:
    id: default_unit_gain_value
    path: \cntk\learners\__init__.py
    remote:
      branch: master
      path: \cntk\learners\__init__.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 62
  summary: Returns true if by default momentum is applied in the unit-gain fashion.
  syntax:
    content: default_unit_gain_value()
  type: Method
  uid: cntk.learners.default_unit_gain_value
- _type: function
  fullName: cntk.learners.default_use_mean_gradient_value
  langs:
  - python
  module: cntk.learners
  name: default_use_mean_gradient_value
  source:
    id: default_use_mean_gradient_value
    path: \cntk\learners\__init__.py
    remote:
      branch: master
      path: \cntk\learners\__init__.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 75
  summary: Returns true if by default input gradient to learner is averaged.
  syntax:
    content: default_use_mean_gradient_value()
  type: Method
  uid: cntk.learners.default_use_mean_gradient_value
- _type: function
  fullName: cntk.learners.fsadagrad
  langs:
  - python
  module: cntk.learners
  name: fsadagrad
  source:
    id: fsadagrad
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 686
  summary: Creates an FSAdaGrad learner instance to learn the parameters.
  syntax:
    content: fsadagrad()
    parameters: &id007
    - description: list of network parameters to tune. These can be obtained by the
        root operator's `parameters`.
      id: parameters
      type:
      - list of parameters
    - description: learning rate schedule.
      id: lr
      type:
      - output of learning_rate_schedule()
    - description: momentum schedule. For additional information, please refer to
        the >>:cntkwiki:`this CNTK Wiki article <BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits>`<<.
      id: momentum
      type:
      - output of momentum_schedule() or momentum_as_time_constant_schedule()
    - description: when `True`, momentum is interpreted as a unit-gain filter. Defaults
        to the value returned by @cntk.learners.default_unit_gain_value.
      id: unit_gain
    - description: variance momentum schedule. Defaults to `momentum_as_time_constant_schedule(720000)`.
      id: variance_momentum
      type:
      - output of momentum_schedule() or momentum_as_time_constant_schedule()
    - description: the L1 regularization weight per sample, defaults to 0.0
      id: l1_regularization_weight
      type:
      - float, optional
    - description: the L2 regularization weight per sample, defaults to 0.0
      id: l2_regularization_weight
      type:
      - float, optional
    - description: the standard deviation of the Gaussian noise added to parameters
        post update, defaults to 0.0
      id: gaussian_noise_injection_std_dev
      type:
      - float, optional
    - description: clipping threshold per sample, defaults to infinity
      id: gradient_clipping_threshold_per_sample
      type:
      - float, optional
    - description: use gradient clipping with truncation
      id: gradient_clipping_with_truncation
      type:
      - bool, default True
    - description: use averaged gradient as input to learner. Defaults to the value
        returned by @cntk.learners.default_use_mean_gradient_value.
      id: use_mean_gradient
      type:
      - bool, default False
    return: &id008
      description: Instance of a @cntk.learners.Learner that can be passed to the
        `Trainer`
  type: Method
  uid: cntk.learners.fsadagrad
- _type: function
  fullName: cntk.learners.learning_rate_schedule
  langs:
  - python
  module: cntk.learners
  name: learning_rate_schedule
  seealsoContent: 'See also: [training_parameter_schedule()](back_rst/cntk.learners.html.md#cntk.learners.training_parameter_schedule.md)'
  source:
    id: learning_rate_schedule
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 308
  summary: Create a learning rate schedule (using the same semantics as [training_parameter_schedule()](back_rst/cntk.learners.html.md#cntk.learners.training_parameter_schedule.md)).
  syntax:
    content: learning_rate_schedule()
    parameters: &id009
    - description: see parameter `schedule` in [training_parameter_schedule()](back_rst/cntk.learners.html.md#cntk.learners.training_parameter_schedule.md).
      id: lr
      type:
      - float or list
    - description: see parameter `unit` in [training_parameter_schedule()](back_rst/cntk.learners.html.md#cntk.learners.training_parameter_schedule.md).
      id: unit
      type:
      - UnitType
    - description: see parameter `epoch_size` in [training_parameter_schedule()](back_rst/cntk.learners.html.md#cntk.learners.training_parameter_schedule.md).
      id: epoch_size
      type:
      - int
    return: &id010
      description: learning rate schedule
  type: Method
  uid: cntk.learners.learning_rate_schedule
- _type: function
  fullName: cntk.learners.momentum_as_time_constant_schedule
  langs:
  - python
  module: cntk.learners
  name: momentum_as_time_constant_schedule
  source:
    id: momentum_as_time_constant_schedule
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 369
  summary: 'Create a momentum schedule in a minibatch-size agnostic way (using the
    same semantics as [training_parameter_schedule()](back_rst/cntk.learners.html.md#cntk.learners.training_parameter_schedule.md)
    with *unit=UnitType.sample*).

    CNTK specifies momentum in a minibatch-size agnostic way as the time constant
    (in samples) of a unit-gain 1st-order IIR filter. The value specifies the number
    of samples after which a gradient has an effect of 1/e=37%.

    If you want to specify the momentum per sample (or per minibatch), use [momentum_schedule()](back_rst/cntk.learners.html.md#cntk.learners.momentum_schedule.md).

    -[ Examples ]-

    >>> # Use a fixed momentum of 1100 for all samples

    >>> m = momentum_as_time_constant_schedule(1100)

    >>> # Use the time constant 1100 for the first 1000 samples,

    >>> # then 1500 for the remaining ones

    >>> m = momentum_as_time_constant_schedule([1100, 1500], 1000)'
  syntax:
    content: momentum_as_time_constant_schedule()
    return: &id011
      description: momentum as time constant schedule
  type: Method
  uid: cntk.learners.momentum_as_time_constant_schedule
- _type: function
  fullName: cntk.learners.momentum_schedule
  langs:
  - python
  module: cntk.learners
  name: momentum_schedule
  source:
    id: momentum_schedule
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 331
  summary: 'Create a per-minibatch momentum schedule (using the same semantics as
    [training_parameter_schedule()](back_rst/cntk.learners.html.md#cntk.learners.training_parameter_schedule.md)
    with the *unit=UnitType.minibatch*).

    If you want to provide momentum values in a minibatch-size agnostic way, use @cntk.learners.momentum_as_time_constant_schedule.

    -[ Examples ]-

    >>> # Use a fixed momentum of 0.99 for all samples

    >>> m = momentum_schedule(0.99)

    >>> # Use the momentum value 0.99 for the first 1000 samples,

    >>> # then 0.9 for the remaining ones

    >>> m = momentum_schedule([0.99,0.9], 1000)

    >>> m[0], m[999], m[1000], m[1001]

    (0.99, 0.99, 0.9, 0.9)

    >>> # Use the momentum value 0.99 for the first 999 samples,

    >>> # then 0.88 for the next 888 samples, and 0.77 for the

    >>> # the remaining ones

    >>> m = momentum_schedule([(999,0.99),(888,0.88),(0, 0.77)])

    >>> m[0], m[998], m[999], m[999+888-1], m[999+888]

    (0.99, 0.99, 0.88, 0.88, 0.77)'
  syntax:
    content: momentum_schedule()
    return: &id012
      description: momentum schedule
  type: Method
  uid: cntk.learners.momentum_schedule
- _type: function
  fullName: cntk.learners.momentum_sgd
  langs:
  - python
  module: cntk.learners
  name: momentum_sgd
  source:
    id: momentum_sgd
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 470
  summary: Creates a Momentum SGD learner instance to learn the parameters.
  syntax:
    content: momentum_sgd()
    parameters: &id013
    - description: list of network parameters to tune. These can be obtained by the
        root operator's `parameters`.
      id: parameters
      type:
      - list of parameters
    - description: learning rate schedule.
      id: lr
      type:
      - output of learning_rate_schedule()
    - description: momentum schedule. For additional information, please refer to
        the >>:cntkwiki:`this CNTK Wiki article <BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits>`<<.
      id: momentum
      type:
      - output of momentum_schedule() or momentum_as_time_constant_schedule()
    - description: when `True`, momentum is interpreted as a unit-gain filter. Defaults
        to the value returned by @cntk.learners.default_unit_gain_value.
      id: unit_gain
    - description: the L1 regularization weight per sample, defaults to 0.0
      id: l1_regularization_weight
      type:
      - float, optional
    - description: the L2 regularization weight per sample, defaults to 0.0
      id: l2_regularization_weight
      type:
      - float, optional
    - description: the standard deviation of the Gaussian noise added to parameters
        post update, defaults to 0.0
      id: gaussian_noise_injection_std_dev
      type:
      - float, optional
    - description: clipping threshold per sample, defaults to infinity
      id: gradient_clipping_threshold_per_sample
      type:
      - float, optional
    - description: use gradient clipping with truncation
      id: gradient_clipping_with_truncation
      type:
      - bool, default True
    - description: use averaged gradient as input to learner. Defaults to the value
        returned by @cntk.learners.default_use_mean_gradient_value.
      id: use_mean_gradient
      type:
      - bool, default False
    return: &id014
      description: Instance of a @cntk.learners.Learner that can be passed to the
        `Trainer`
  type: Method
  uid: cntk.learners.momentum_sgd
- _type: function
  fullName: cntk.learners.nesterov
  langs:
  - python
  module: cntk.learners
  name: nesterov
  seealsoContent: "See also: [1] Y. Nesterov. A Method of Solving a Convex Programming\
    \ Problem with Convergence Rate O(1/ sqrt(k)). Soviet Mathematics Doklady, 1983.\n\
    \n  [2] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. [On the Importance of\
    \ Initialization and Momentum in Deep Learning](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf).\
    \  Proceedings of the 30th International Conference on Machine Learning, 2013."
  source:
    id: nesterov
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 521
  summary: Creates a Nesterov SGD learner instance to learn the parameters. This was
    originally proposed by Nesterov [1] in 1983 and then shown to work well in a deep
    learning context by Sutskever, et al. [2].
  syntax:
    content: nesterov()
    parameters: &id015
    - description: list of network parameters to tune. These can be obtained by the
        root operator's `parameters`.
      id: parameters
      type:
      - list of parameters
    - description: learning rate schedule.
      id: lr
      type:
      - output of learning_rate_schedule()
    - description: momentum schedule. For additional information, please refer to
        the >>:cntkwiki:`this CNTK Wiki article <BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits>`<<.
      id: momentum
      type:
      - output of momentum_schedule() or momentum_as_time_constant_schedule()
    - description: when `True`, momentum is interpreted as a unit-gain filter. Defaults
        to the value returned by @cntk.learners.default_unit_gain_value.
      id: unit_gain
    - description: the L1 regularization weight per sample, defaults to 0.0
      id: l1_regularization_weight
      type:
      - float, optional
    - description: the L2 regularization weight per sample, defaults to 0.0
      id: l2_regularization_weight
      type:
      - float, optional
    - description: the standard deviation of the Gaussian noise added to parameters
        post update, defaults to 0.0
      id: gaussian_noise_injection_std_dev
      type:
      - float, optional
    - description: clipping threshold per sample, defaults to infinity
      id: gradient_clipping_threshold_per_sample
      type:
      - float, optional
    - description: use gradient clipping with truncation
      id: gradient_clipping_with_truncation
      type:
      - bool, default True
    - description: use averaged gradient as input to learner. Defaults to the value
        returned by @cntk.learners.default_use_mean_gradient_value.
      id: use_mean_gradient
      type:
      - bool, default False
    return: &id016
      description: Instance of a @cntk.learners.Learner that can be passed to the
        `Trainer`.
  type: Method
  uid: cntk.learners.nesterov
- _type: function
  fullName: cntk.learners.rmsprop
  langs:
  - python
  module: cntk.learners
  name: rmsprop
  source:
    id: rmsprop
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 801
  summary: Creates an RMSProp learner instance to learn the parameters.
  syntax:
    content: rmsprop()
    parameters: &id017
    - description: list of network parameters to tune. These can be obtained by the
        root operator's `parameters`.
      id: parameters
      type:
      - list of parameters
    - description: learning rate schedule.
      id: lr
      type:
      - output of learning_rate_schedule()
    - description: Trade-off factor for current and previous gradients. Common value
        is 0.95
      id: gamma
      type:
      - float
    - description: Increasing factor when trying to adjust current learning_rate
      id: inc
      type:
      - float
    - description: Decreasing factor when trying to adjust current learning_rate
      id: dec
      type:
      - float
    - description: Maximum scale allowed for the initial learning_rate
      id: max
      type:
      - float
    - description: Minimum scale allowed for the initial learning_rate
      id: min
      type:
      - float
    - description: ''
      id: need_ave_multiplier
      type:
      - bool, default True
    - description: the L1 regularization weight per sample, defaults to 0.0
      id: l1_regularization_weight
      type:
      - float, optional
    - description: the L2 regularization weight per sample, defaults to 0.0
      id: l2_regularization_weight
      type:
      - float, optional
    - description: the standard deviation of the Gaussian noise added to parameters
        post update, defaults to 0.0
      id: gaussian_noise_injection_std_dev
      type:
      - float, optional
    - description: clipping threshold per sample, defaults to infinity
      id: gradient_clipping_threshold_per_sample
      type:
      - float, optional
    - description: use gradient clipping with truncation
      id: gradient_clipping_with_truncation
      type:
      - bool, default True
    - description: use averaged gradient as input to learner. Defaults to the value
        returned by @cntk.learners.default_use_mean_gradient_value.
      id: use_mean_gradient
      type:
      - bool, default False
    return: &id018
      description: Instance of a @cntk.learners.Learner that can be passed to the
        `Trainer`
  type: Method
  uid: cntk.learners.rmsprop
- _type: function
  fullName: cntk.learners.set_default_unit_gain_value
  langs:
  - python
  module: cntk.learners
  name: set_default_unit_gain_value
  source:
    id: set_default_unit_gain_value
    path: \cntk\learners\__init__.py
    remote:
      branch: master
      path: \cntk\learners\__init__.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 69
  summary: Sets globally default unit-gain flag value.
  syntax:
    content: set_default_unit_gain_value(value)
    parameters:
    - id: value
  type: Method
  uid: cntk.learners.set_default_unit_gain_value
- _type: function
  fullName: cntk.learners.set_default_use_mean_gradient_value
  langs:
  - python
  module: cntk.learners
  name: set_default_use_mean_gradient_value
  source:
    id: set_default_use_mean_gradient_value
    path: \cntk\learners\__init__.py
    remote:
      branch: master
      path: \cntk\learners\__init__.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 82
  summary: Sets globally default use_mean_gradient_value.
  syntax:
    content: set_default_use_mean_gradient_value(value)
    parameters:
    - id: value
  type: Method
  uid: cntk.learners.set_default_use_mean_gradient_value
- _type: function
  fullName: cntk.learners.sgd
  langs:
  - python
  module: cntk.learners
  name: sgd
  seealsoContent: 'See also: [1] L. Bottou. [Stochastic Gradient Descent Tricks](https://www.microsoft.com/en-us/research/publication/stochastic-gradient-tricks).
    Neural Networks: Tricks of the Trade: Springer, 2012.'
  source:
    id: sgd
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 419
  summary: Creates an SGD learner instance to learn the parameters. See [1] for more
    information on how to set the parameters.
  syntax:
    content: sgd()
    parameters: &id019
    - description: list of network parameters to tune. These can be obtained by the
        '.parameters()' method of the root operator.
      id: parameters
      type:
      - list of parameters
    - description: learning rate schedule.
      id: lr
      type:
      - output of learning_rate_schedule()
    - description: the L1 regularization weight per sample, defaults to 0.0
      id: l1_regularization_weight
      type:
      - float, optional
    - description: the L2 regularization weight per sample, defaults to 0.0
      id: l2_regularization_weight
      type:
      - float, optional
    - description: the standard deviation of the Gaussian noise added to parameters
        post update, defaults to 0.0
      id: gaussian_noise_injection_std_dev
      type:
      - float, optional
    - description: clipping threshold per sample, defaults to infinity
      id: gradient_clipping_threshold_per_sample
      type:
      - float, optional
    - description: use gradient clipping with truncation
      id: gradient_clipping_with_truncation
      type:
      - bool, default True
    - description: use averaged gradient as input to learner. Defaults to the value
        returned by @cntk.learners.default_use_mean_gradient_value.
      id: use_mean_gradient
      type:
      - bool, default False
    return: &id020
      description: Instance of a @cntk.learners.Learner that can be passed to the
        `Trainer`
  type: Method
  uid: cntk.learners.sgd
- _type: function
  fullName: cntk.learners.training_parameter_schedule
  langs:
  - python
  module: cntk.learners
  name: training_parameter_schedule
  seealsoContent: 'See also: @cntk.learners.learning_rate_schedule'
  source:
    id: training_parameter_schedule
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 236
  summary: 'Create a training parameter schedule containing either per-sample (default)
    or per-minibatch values.

    -[ Examples ]-

    >>> # Use a fixed value 0.01 for all samples

    >>> s = training_parameter_schedule(0.01, UnitType.sample)

    >>> s[0], s[1]

    (0.01, 0.01)

    >>> # Use 0.01 for the first 1000 samples, then 0.001 for the remaining ones

    >>> s = training_parameter_schedule([0.01, 0.001], UnitType.sample, 1000)

    >>> s[0], s[1], s[1000], s[1001]

    (0.01, 0.01, 0.001, 0.001)

    >>> # Use 0.1 for the first 12 epochs, then 0.01 for the next 15,

    >>> # followed by 0.001 for the remaining ones, with a 100 samples in an epoch

    >>> s = training_parameter_schedule([(12, 0.1), (15, 0.01), (1, 0.001)], UnitType.sample,
    100)

    >>> s[0], s[1199], s[1200], s[2699], s[2700], s[5000]

    (0.1, 0.1, 0.01, 0.01, 0.001, 0.001)'
  syntax:
    content: training_parameter_schedule()
    parameters: &id021
    - description: if float, is the parameter schedule to be used for all samples.
        In case of list, the elements are used as the values for `epoch_size` samples.
        If list contains pair, the second element is used as a value for (`epoch_size`
        x first element) samples
      id: schedule
      type:
      - float or list
    - description: 'one of two * `sample`: the returned schedule contains per-sample
        values * `minibatch`: the returned schedule contains per-minibatch values.'
      id: unit
      type:
      - UnitType
    - description: number of samples as a scheduling unit. Parameters in the schedule
        change their values every `epoch_size` samples. If no `epoch_size` is provided,
        this parameter is substituted by the size of the full data sweep, in which
        case the scheduling unit is the entire data sweep (as indicated by the MinibatchSource)
        and parameters change their values on the sweep-by-sweep basis specified by
        the `schedule`.
      id: epoch_size
      type:
      - optional, int
    return: &id022
      description: training parameter schedule
  type: Method
  uid: cntk.learners.training_parameter_schedule
- _type: module
  children: []
  fullName: cntk.learners
  langs:
  - python
  module: cntk.learners
  name: learners
  source:
    id: learners
    path: \cntk\learners\__init__.py
    remote:
      branch: master
      path: \cntk\learners\__init__.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 0
  summary: 'Learner tunes a set of parameters during the training process. One can
    use different learners for different sets of parameters. Currently, CNTK supports
    the following learning algorithms:


    +------------------------+ | Learning algorithms    | +========================+
    | AdaDelta               | +------------------------+ | AdaGrad                |
    +------------------------+ | FSAdaGrad              | +------------------------+
    | Adam                   | +------------------------+ | MomentumSGD            |
    +------------------------+ | Nesterov               | +------------------------+
    | RMSProp                | +------------------------+ | SGD                    |
    +------------------------+'
  type: Namespace
  uid: cntk.learners
- _type: function
  fullName: cntk.learners.adadelta
  langs:
  - python
  module: cntk.learners
  name: adadelta
  seealsoContent: 'See also: [1]  Matthew D. Zeiler1, [ADADELTA: AN ADAPTIVE LEARNING
    RATE METHOD](https://arxiv.org/pdf/1212.5701.pdf).'
  source:
    id: adadelta
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 582
  summary: Creates an AdaDelta learner instance to learn the parameters. See [1] for
    more information.
  syntax:
    content: adadelta()
    parameters: *id001
    return: *id002
  type: Method
  uid: cntk.learners.adadelta
- _type: function
  fullName: cntk.learners.adagrad
  langs:
  - python
  module: cntk.learners
  name: adagrad
  seealsoContent: 'See also: [1]  J. Duchi, E. Hazan, and Y. Singer. [Adaptive Subgradient
    Methods for Online Learning and Stochastic Optimization](http://www.magicbroom.info/Papers/DuchiHaSi10.pdf).
    The Journal of Machine Learning Research, 2011.'
  source:
    id: adagrad
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 633
  summary: Creates an AdaGrad learner instance to learn the parameters. See [1] for
    more information.
  syntax:
    content: adagrad()
    parameters: *id003
    return: *id004
  type: Method
  uid: cntk.learners.adagrad
- _type: function
  fullName: cntk.learners.adam
  langs:
  - python
  module: cntk.learners
  name: adam
  seealsoContent: 'See also: [1] D. Kingma, J. Ba. [Adam: A Method for Stochastic
    Optimization](https://arxiv.org/abs/1412.6980). International Conference for Learning
    Representations, 2015.'
  source:
    id: adam
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 741
  summary: Creates an Adam learner instance to learn the parameters. See [1] for more
    information.
  syntax:
    content: adam()
    parameters: *id005
    return: *id006
  type: Method
  uid: cntk.learners.adam
- _type: function
  fullName: cntk.learners.default_unit_gain_value
  langs:
  - python
  module: cntk.learners
  name: default_unit_gain_value
  source:
    id: default_unit_gain_value
    path: \cntk\learners\__init__.py
    remote:
      branch: master
      path: \cntk\learners\__init__.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 62
  summary: Returns true if by default momentum is applied in the unit-gain fashion.
  syntax:
    content: default_unit_gain_value()
  type: Method
  uid: cntk.learners.default_unit_gain_value
- _type: function
  fullName: cntk.learners.default_use_mean_gradient_value
  langs:
  - python
  module: cntk.learners
  name: default_use_mean_gradient_value
  source:
    id: default_use_mean_gradient_value
    path: \cntk\learners\__init__.py
    remote:
      branch: master
      path: \cntk\learners\__init__.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 75
  summary: Returns true if by default input gradient to learner is averaged.
  syntax:
    content: default_use_mean_gradient_value()
  type: Method
  uid: cntk.learners.default_use_mean_gradient_value
- _type: function
  fullName: cntk.learners.fsadagrad
  langs:
  - python
  module: cntk.learners
  name: fsadagrad
  source:
    id: fsadagrad
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 686
  summary: Creates an FSAdaGrad learner instance to learn the parameters.
  syntax:
    content: fsadagrad()
    parameters: *id007
    return: *id008
  type: Method
  uid: cntk.learners.fsadagrad
- _type: function
  fullName: cntk.learners.learning_rate_schedule
  langs:
  - python
  module: cntk.learners
  name: learning_rate_schedule
  seealsoContent: 'See also: [training_parameter_schedule()](back_rst/cntk.learners.html.md#cntk.learners.training_parameter_schedule.md)'
  source:
    id: learning_rate_schedule
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 308
  summary: Create a learning rate schedule (using the same semantics as [training_parameter_schedule()](back_rst/cntk.learners.html.md#cntk.learners.training_parameter_schedule.md)).
  syntax:
    content: learning_rate_schedule()
    parameters: *id009
    return: *id010
  type: Method
  uid: cntk.learners.learning_rate_schedule
- _type: function
  fullName: cntk.learners.momentum_as_time_constant_schedule
  langs:
  - python
  module: cntk.learners
  name: momentum_as_time_constant_schedule
  source:
    id: momentum_as_time_constant_schedule
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 369
  summary: 'Create a momentum schedule in a minibatch-size agnostic way (using the
    same semantics as [training_parameter_schedule()](back_rst/cntk.learners.html.md#cntk.learners.training_parameter_schedule.md)
    with *unit=UnitType.sample*).

    CNTK specifies momentum in a minibatch-size agnostic way as the time constant
    (in samples) of a unit-gain 1st-order IIR filter. The value specifies the number
    of samples after which a gradient has an effect of 1/e=37%.

    If you want to specify the momentum per sample (or per minibatch), use [momentum_schedule()](back_rst/cntk.learners.html.md#cntk.learners.momentum_schedule.md).

    -[ Examples ]-

    >>> # Use a fixed momentum of 1100 for all samples

    >>> m = momentum_as_time_constant_schedule(1100)

    >>> # Use the time constant 1100 for the first 1000 samples,

    >>> # then 1500 for the remaining ones

    >>> m = momentum_as_time_constant_schedule([1100, 1500], 1000)'
  syntax:
    content: momentum_as_time_constant_schedule()
    return: *id011
  type: Method
  uid: cntk.learners.momentum_as_time_constant_schedule
- _type: function
  fullName: cntk.learners.momentum_schedule
  langs:
  - python
  module: cntk.learners
  name: momentum_schedule
  source:
    id: momentum_schedule
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 331
  summary: 'Create a per-minibatch momentum schedule (using the same semantics as
    [training_parameter_schedule()](back_rst/cntk.learners.html.md#cntk.learners.training_parameter_schedule.md)
    with the *unit=UnitType.minibatch*).

    If you want to provide momentum values in a minibatch-size agnostic way, use @cntk.learners.momentum_as_time_constant_schedule.

    -[ Examples ]-

    >>> # Use a fixed momentum of 0.99 for all samples

    >>> m = momentum_schedule(0.99)

    >>> # Use the momentum value 0.99 for the first 1000 samples,

    >>> # then 0.9 for the remaining ones

    >>> m = momentum_schedule([0.99,0.9], 1000)

    >>> m[0], m[999], m[1000], m[1001]

    (0.99, 0.99, 0.9, 0.9)

    >>> # Use the momentum value 0.99 for the first 999 samples,

    >>> # then 0.88 for the next 888 samples, and 0.77 for the

    >>> # the remaining ones

    >>> m = momentum_schedule([(999,0.99),(888,0.88),(0, 0.77)])

    >>> m[0], m[998], m[999], m[999+888-1], m[999+888]

    (0.99, 0.99, 0.88, 0.88, 0.77)'
  syntax:
    content: momentum_schedule()
    return: *id012
  type: Method
  uid: cntk.learners.momentum_schedule
- _type: function
  fullName: cntk.learners.momentum_sgd
  langs:
  - python
  module: cntk.learners
  name: momentum_sgd
  source:
    id: momentum_sgd
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 470
  summary: Creates a Momentum SGD learner instance to learn the parameters.
  syntax:
    content: momentum_sgd()
    parameters: *id013
    return: *id014
  type: Method
  uid: cntk.learners.momentum_sgd
- _type: function
  fullName: cntk.learners.nesterov
  langs:
  - python
  module: cntk.learners
  name: nesterov
  seealsoContent: "See also: [1] Y. Nesterov. A Method of Solving a Convex Programming\
    \ Problem with Convergence Rate O(1/ sqrt(k)). Soviet Mathematics Doklady, 1983.\n\
    \n  [2] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. [On the Importance of\
    \ Initialization and Momentum in Deep Learning](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf).\
    \  Proceedings of the 30th International Conference on Machine Learning, 2013."
  source:
    id: nesterov
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 521
  summary: Creates a Nesterov SGD learner instance to learn the parameters. This was
    originally proposed by Nesterov [1] in 1983 and then shown to work well in a deep
    learning context by Sutskever, et al. [2].
  syntax:
    content: nesterov()
    parameters: *id015
    return: *id016
  type: Method
  uid: cntk.learners.nesterov
- _type: function
  fullName: cntk.learners.rmsprop
  langs:
  - python
  module: cntk.learners
  name: rmsprop
  source:
    id: rmsprop
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 801
  summary: Creates an RMSProp learner instance to learn the parameters.
  syntax:
    content: rmsprop()
    parameters: *id017
    return: *id018
  type: Method
  uid: cntk.learners.rmsprop
- _type: function
  fullName: cntk.learners.set_default_unit_gain_value
  langs:
  - python
  module: cntk.learners
  name: set_default_unit_gain_value
  source:
    id: set_default_unit_gain_value
    path: \cntk\learners\__init__.py
    remote:
      branch: master
      path: \cntk\learners\__init__.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 69
  summary: Sets globally default unit-gain flag value.
  syntax:
    content: set_default_unit_gain_value(value)
    parameters:
    - id: value
  type: Method
  uid: cntk.learners.set_default_unit_gain_value
- _type: function
  fullName: cntk.learners.set_default_use_mean_gradient_value
  langs:
  - python
  module: cntk.learners
  name: set_default_use_mean_gradient_value
  source:
    id: set_default_use_mean_gradient_value
    path: \cntk\learners\__init__.py
    remote:
      branch: master
      path: \cntk\learners\__init__.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 82
  summary: Sets globally default use_mean_gradient_value.
  syntax:
    content: set_default_use_mean_gradient_value(value)
    parameters:
    - id: value
  type: Method
  uid: cntk.learners.set_default_use_mean_gradient_value
- _type: function
  fullName: cntk.learners.sgd
  langs:
  - python
  module: cntk.learners
  name: sgd
  seealsoContent: 'See also: [1] L. Bottou. [Stochastic Gradient Descent Tricks](https://www.microsoft.com/en-us/research/publication/stochastic-gradient-tricks).
    Neural Networks: Tricks of the Trade: Springer, 2012.'
  source:
    id: sgd
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 419
  summary: Creates an SGD learner instance to learn the parameters. See [1] for more
    information on how to set the parameters.
  syntax:
    content: sgd()
    parameters: *id019
    return: *id020
  type: Method
  uid: cntk.learners.sgd
- _type: function
  fullName: cntk.learners.training_parameter_schedule
  langs:
  - python
  module: cntk.learners
  name: training_parameter_schedule
  seealsoContent: 'See also: @cntk.learners.learning_rate_schedule'
  source:
    id: training_parameter_schedule
    path: \cntk\internal\swig_helper.py
    remote:
      branch: master
      path: \cntk\internal\swig_helper.py
      repo: https://github.com/Microsoft/CNTK.git
    startLine: 236
  summary: 'Create a training parameter schedule containing either per-sample (default)
    or per-minibatch values.

    -[ Examples ]-

    >>> # Use a fixed value 0.01 for all samples

    >>> s = training_parameter_schedule(0.01, UnitType.sample)

    >>> s[0], s[1]

    (0.01, 0.01)

    >>> # Use 0.01 for the first 1000 samples, then 0.001 for the remaining ones

    >>> s = training_parameter_schedule([0.01, 0.001], UnitType.sample, 1000)

    >>> s[0], s[1], s[1000], s[1001]

    (0.01, 0.01, 0.001, 0.001)

    >>> # Use 0.1 for the first 12 epochs, then 0.01 for the next 15,

    >>> # followed by 0.001 for the remaining ones, with a 100 samples in an epoch

    >>> s = training_parameter_schedule([(12, 0.1), (15, 0.01), (1, 0.001)], UnitType.sample,
    100)

    >>> s[0], s[1199], s[1200], s[2699], s[2700], s[5000]

    (0.1, 0.1, 0.01, 0.01, 0.001, 0.001)'
  syntax:
    content: training_parameter_schedule()
    parameters: *id021
    return: *id022
  type: Method
  uid: cntk.learners.training_parameter_schedule
references:
- fullName: cntk.learners.Learner
  isExternal: false
  name: Learner
  parent: cntk.learners
  uid: cntk.learners.Learner
- fullName: cntk.learners.UnitType
  isExternal: false
  name: UnitType
  parent: cntk.learners
  uid: cntk.learners.UnitType
- fullName: cntk.learners.UserLearner
  isExternal: false
  name: UserLearner
  parent: cntk.learners
  uid: cntk.learners.UserLearner
- fullName: cntk.learners.adadelta
  isExternal: false
  name: adadelta
  parent: cntk.learners
  uid: cntk.learners.adadelta
- fullName: cntk.learners.adagrad
  isExternal: false
  name: adagrad
  parent: cntk.learners
  uid: cntk.learners.adagrad
- fullName: cntk.learners.adam
  isExternal: false
  name: adam
  parent: cntk.learners
  uid: cntk.learners.adam
- fullName: cntk.learners.default_unit_gain_value
  isExternal: false
  name: default_unit_gain_value
  parent: cntk.learners
  uid: cntk.learners.default_unit_gain_value
- fullName: cntk.learners.default_use_mean_gradient_value
  isExternal: false
  name: default_use_mean_gradient_value
  parent: cntk.learners
  uid: cntk.learners.default_use_mean_gradient_value
- fullName: cntk.learners.fsadagrad
  isExternal: false
  name: fsadagrad
  parent: cntk.learners
  uid: cntk.learners.fsadagrad
- fullName: cntk.learners.learning_rate_schedule
  isExternal: false
  name: learning_rate_schedule
  parent: cntk.learners
  uid: cntk.learners.learning_rate_schedule
- fullName: cntk.learners.momentum_as_time_constant_schedule
  isExternal: false
  name: momentum_as_time_constant_schedule
  parent: cntk.learners
  uid: cntk.learners.momentum_as_time_constant_schedule
- fullName: cntk.learners.momentum_schedule
  isExternal: false
  name: momentum_schedule
  parent: cntk.learners
  uid: cntk.learners.momentum_schedule
- fullName: cntk.learners.momentum_sgd
  isExternal: false
  name: momentum_sgd
  parent: cntk.learners
  uid: cntk.learners.momentum_sgd
- fullName: cntk.learners.nesterov
  isExternal: false
  name: nesterov
  parent: cntk.learners
  uid: cntk.learners.nesterov
- fullName: cntk.learners.rmsprop
  isExternal: false
  name: rmsprop
  parent: cntk.learners
  uid: cntk.learners.rmsprop
- fullName: cntk.learners.set_default_unit_gain_value
  isExternal: false
  name: set_default_unit_gain_value
  parent: cntk.learners
  uid: cntk.learners.set_default_unit_gain_value
- fullName: cntk.learners.set_default_use_mean_gradient_value
  isExternal: false
  name: set_default_use_mean_gradient_value
  parent: cntk.learners
  uid: cntk.learners.set_default_use_mean_gradient_value
- fullName: cntk.learners.sgd
  isExternal: false
  name: sgd
  parent: cntk.learners
  uid: cntk.learners.sgd
- fullName: cntk.learners.training_parameter_schedule
  isExternal: false
  name: training_parameter_schedule
  parent: cntk.learners
  uid: cntk.learners.training_parameter_schedule
- fullName: cntk.learners.Learner
  isExternal: false
  name: Learner
  parent: cntk.learners
  uid: cntk.learners.Learner
- fullName: cntk.learners.UnitType
  isExternal: false
  name: UnitType
  parent: cntk.learners
  uid: cntk.learners.UnitType
- fullName: cntk.learners.UserLearner
  isExternal: false
  name: UserLearner
  parent: cntk.learners
  uid: cntk.learners.UserLearner
- fullName: cntk.learners.adadelta
  isExternal: false
  name: adadelta
  parent: cntk.learners
  uid: cntk.learners.adadelta
- fullName: cntk.learners.adagrad
  isExternal: false
  name: adagrad
  parent: cntk.learners
  uid: cntk.learners.adagrad
- fullName: cntk.learners.adam
  isExternal: false
  name: adam
  parent: cntk.learners
  uid: cntk.learners.adam
- fullName: cntk.learners.default_unit_gain_value
  isExternal: false
  name: default_unit_gain_value
  parent: cntk.learners
  uid: cntk.learners.default_unit_gain_value
- fullName: cntk.learners.default_use_mean_gradient_value
  isExternal: false
  name: default_use_mean_gradient_value
  parent: cntk.learners
  uid: cntk.learners.default_use_mean_gradient_value
- fullName: cntk.learners.fsadagrad
  isExternal: false
  name: fsadagrad
  parent: cntk.learners
  uid: cntk.learners.fsadagrad
- fullName: cntk.learners.learning_rate_schedule
  isExternal: false
  name: learning_rate_schedule
  parent: cntk.learners
  uid: cntk.learners.learning_rate_schedule
- fullName: cntk.learners.momentum_as_time_constant_schedule
  isExternal: false
  name: momentum_as_time_constant_schedule
  parent: cntk.learners
  uid: cntk.learners.momentum_as_time_constant_schedule
- fullName: cntk.learners.momentum_schedule
  isExternal: false
  name: momentum_schedule
  parent: cntk.learners
  uid: cntk.learners.momentum_schedule
- fullName: cntk.learners.momentum_sgd
  isExternal: false
  name: momentum_sgd
  parent: cntk.learners
  uid: cntk.learners.momentum_sgd
- fullName: cntk.learners.nesterov
  isExternal: false
  name: nesterov
  parent: cntk.learners
  uid: cntk.learners.nesterov
- fullName: cntk.learners.rmsprop
  isExternal: false
  name: rmsprop
  parent: cntk.learners
  uid: cntk.learners.rmsprop
- fullName: cntk.learners.set_default_unit_gain_value
  isExternal: false
  name: set_default_unit_gain_value
  parent: cntk.learners
  uid: cntk.learners.set_default_unit_gain_value
- fullName: cntk.learners.set_default_use_mean_gradient_value
  isExternal: false
  name: set_default_use_mean_gradient_value
  parent: cntk.learners
  uid: cntk.learners.set_default_use_mean_gradient_value
- fullName: cntk.learners.sgd
  isExternal: false
  name: sgd
  parent: cntk.learners
  uid: cntk.learners.sgd
- fullName: cntk.learners.training_parameter_schedule
  isExternal: false
  name: training_parameter_schedule
  parent: cntk.learners
  uid: cntk.learners.training_parameter_schedule
